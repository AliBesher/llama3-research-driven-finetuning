{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ØªØ¬Ø±Ø¨Ù‡ Ù†Ù‡Ø§Ø¦ÙŠÙ‡  Ø²Ø¯Ù†Ø§ Ø§Ù„Ø¯Ø§ØªØ§ Ù„ 50 Ø§Ù„Ù  Ø¹ 500 Ø³ÙŠÙƒØ´ÙŠÙ† Ø¨Ø¹Ø¯ Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù†Ù‡ 700 Ø§ÙØ¶Ù„ Ø¨Ø³ Ø§Ù„ØªØ¬Ø±Ø¨Ø§Ù‡ Ù…Ø´ ØºÙ„Ø·\n",
        "Final experiment: we increased the data to 50,000 across 500 sections. After logical consideration, 700 would be better, but the experiment is not wrong."
      ],
      "metadata": {
        "id": "mKNJp4xRa0-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** first exp (in this exp no comments no details - the final experiment, including an explanation of parameter selection and engineering reasoning, is presented in the final  experiment; scroll down to enjoy  it ).**\n",
        "\n",
        "Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø±Ù‚Ù… ÙˆØ§Ø­Ø¯ (Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ù…Ø¹ Ø´Ø±Ø­ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª ÙˆØ§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠ Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©ØŒ Ø§Ù†ØªÙ‚Ù„ Ù„Ù„Ø£Ø³ÙÙ„)."
      ],
      "metadata": {
        "id": "LEiQTfmnqVRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ØªØ­Ù…ÙŠÙ„ Ù…ÙƒØ§ØªØ¨\n",
        "\n",
        "Importing libraries."
      ],
      "metadata": {
        "id": "eA43zCKnbGf5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gS8-F-0Kco-s",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"DISABLE_FLASH_ATTN\"] = \"1\"  # ØªØ¹Ø·ÙŠÙ„ Flash Attention\n",
        "\n",
        "# ØªØ«Ø¨ÙŠØª Unsloth Ø£ÙˆÙ„Ø§Ù‹\n",
        "!pip install --no-deps --upgrade unsloth\n",
        "!pip install --upgrade transformers datasets accelerate peft trl\n",
        "!pip install bitsandbytes\n",
        "!pip install tyro unsloth_zoo xformers\n",
        "!pip install --upgrade huggingface_hub\n",
        "!pip install torch==2.7.0 torchvision==0.18.0 torchaudio==2.7.0 --force-reinstall\n",
        "\n",
        "print(\"âœ… Libraries installed successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Unsloth\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "import gc\n",
        "\n",
        "print(\"All imports successful\")"
      ],
      "metadata": {
        "id": "JswljLMvc3f6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Training settings and the final base model configuration (approximately)\n",
        "Ø§Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§Ø³Ø§Ø³ÙŠ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ØªÙ‚Ø±ÙŠØ¨Ø§\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ckd0cr_SbLIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸ“¥ Loading model...\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    max_seq_length=1024,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    attn_implementation=\"eager\",\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=8,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        ")\n",
        "\n",
        "# Data preparation\n",
        "print(\"ğŸ“Š Preparing data...\")\n",
        "\n",
        "dataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\").select(range(50000))\n",
        "print(f\"ğŸ“š Loaded {len(dataset):,} examples\")\n",
        "\n",
        "def format_conversations(example):\n",
        "    conv = example[\"conversations\"]\n",
        "    text = \"<|begin_of_text|>\"\n",
        "\n",
        "    for turn in conv:\n",
        "        role = turn.get(\"from\", turn.get(\"role\", \"\"))\n",
        "        content = turn.get(\"value\", turn.get(\"content\", \"\"))\n",
        "\n",
        "        if role in [\"human\", \"user\"]:\n",
        "            text += f\"<|start_header_id|>user<|end_header_id|>\\n\\n{content}<|eot_id|>\"\n",
        "        elif role in [\"gpt\", \"assistant\"]:\n",
        "            text += f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{content}<|eot_id|>\"\n",
        "\n",
        "    return {\"text\": text}\n",
        "\n",
        "dataset = dataset.map(format_conversations)\n",
        "dataset = dataset.filter(lambda x: len(x[\"text\"]) > 100)\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "train_dataset = dataset.select(range(train_size))\n",
        "eval_dataset = dataset.select(range(train_size, len(dataset)))\n",
        "\n",
        "print(f\"âœ… Training: {len(train_dataset)}, Eval: {len(eval_dataset)}\")\n",
        "\n",
        "# Training setup\n",
        "print(\"ğŸ‹ï¸ Setting up training...\")\n",
        "\n",
        "from transformers import TrainingArguments, TrainerCallback\n",
        "\n",
        "class MovingAverageEvalLossCallback(TrainerCallback):\n",
        "    def __init__(self, window_size=10):\n",
        "        self.window_size = window_size\n",
        "        self.eval_losses = []\n",
        "\n",
        "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "        if metrics and \"eval_loss\" in metrics:\n",
        "            self.eval_losses.append(metrics[\"eval_loss\"])\n",
        "            self.eval_losses = self.eval_losses[-self.window_size:]\n",
        "            avg = sum(self.eval_losses) / len(self.eval_losses)\n",
        "            print(f\"\\nğŸ”¹ Ù…ØªÙˆØ³Ø· Ø¢Ø®Ø± {len(self.eval_losses)} eval_loss: {avg:.4f}\\n\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./llama3_intensive\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    per_device_eval_batch_size=2,\n",
        "    learning_rate=1e-4,\n",
        "    warmup_steps=100,\n",
        "    max_steps=500,\n",
        "    eval_steps=50,\n",
        "    save_steps=200,\n",
        "    logging_steps=25,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    dataloader_num_workers=2,\n",
        "    remove_unused_columns=True,\n",
        "    report_to=\"none\",\n",
        "    seed=42,\n",
        "    gradient_checkpointing=True,\n",
        "    save_total_limit=3,\n",
        ")\n",
        "\n",
        "trainer_intensive = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=1024,\n",
        "    args=training_args,\n",
        "    packing=False,\n",
        "    callbacks=[MovingAverageEvalLossCallback(window_size=10)],\n",
        ")\n",
        "\n",
        "print(\"âœ… Intensive trainer configured!\")\n",
        "print(f\"ğŸ¯ Training plan:\")\n",
        "print(f\"   ğŸ“Š Data: {len(train_dataset):,} examples\")\n",
        "print(f\"   ğŸ”„ Steps:500 (vs 50 before)\")\n",
        "print(f\"   âš¡ Batch: 2Ã—8=16 effective (vs 1Ã—4=4 before)\")\n",
        "print(f\"   â±ï¸  Estimated time: 30-45 minutes\")\n"
      ],
      "metadata": {
        "id": "0oX3ImTBdQ2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After preparing everything and augmenting the data, we start the training."
      ],
      "metadata": {
        "id": "bLAyZ44wbS6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6ï¸âƒ£ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙƒØ«Ù\n",
        "print(\"\\nğŸš€ Starting INTENSIVE training...\")\n",
        "print(\"ğŸ’¡ This will take 30-45 minutes - grab a coffee! â˜•\")\n",
        "\n",
        "try:\n",
        "    # Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨\n",
        "    result_intensive = trainer_intensive.train()\n",
        "\n",
        "    print(\"\\nğŸ‰ INTENSIVE TRAINING COMPLETED!\")\n",
        "    print(f\"ğŸ“Š Final training loss: {result_intensive.training_loss:.4f}\")\n",
        "    print(f\"â±ï¸  Training time: {result_intensive.metrics['train_runtime']:.1f} seconds\")\n",
        "\n",
        "    #  Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ø³Ù†\n",
        "    print(\"ğŸ’¾ Saving intensively trained model...\")\n",
        "    model.save_pretrained(\"./llama3_intensive_final\")\n",
        "    tokenizer.save_pretrained(\"./llama3_intensive_final\")\n",
        "\n",
        "    print(\"âœ… Intensive model saved!\")\n",
        "\n",
        "    #  Ø§Ø®ØªØ¨Ø§Ø± Ø´Ø§Ù…Ù„\n",
        "    print(\"\\nğŸ§ª Testing intensively trained model...\")\n",
        "\n",
        "    FastLanguageModel.for_inference(model)\n",
        "\n",
        "    test_questions = [\n",
        "        \"What is artificial intelligence?\",\n",
        "        \"Explain machine learning in simple terms.\",\n",
        "        \"How do neural networks work?\",\n",
        "        \"What are the benefits of AI?\",\n",
        "        \"Write a short story about a robot.\"\n",
        "    ]\n",
        "\n",
        "    for i, question in enumerate(test_questions, 1):\n",
        "        print(f\"\\nğŸ¤– Test {i}: {question}\")\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙÙ‚Ø·\n",
        "        answer = response.split(\"assistant<|end_header_id|>\\n\\n\")[-1]\n",
        "        print(f\"ğŸ’¬ Answer: {answer[:200]}...\")\n",
        "\n",
        "        if i >= 2:  # Ø§Ø®ØªØ¨Ø§Ø± Ø£ÙˆÙ„ Ø³Ø¤Ø§Ù„ÙŠÙ† ÙÙ‚Ø· Ù„Ù„ØªÙˆÙÙŠØ± ÙÙŠ Ø§Ù„ÙˆÙ‚Øª\n",
        "            print(\"   ... (ØªÙ… ØªÙ‚Ù„ÙŠØµ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù„Ù„ØªÙˆÙÙŠØ± ÙÙŠ Ø§Ù„ÙˆÙ‚Øª)\")\n",
        "            break\n",
        "\n",
        "    print(\"\\nğŸŠ INTENSIVE TRAINING COMPLETE!\")\n",
        "    print(\"ğŸ“ Model saved in: ./llama3_intensive_final/\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Training error: {e}\")\n",
        "    print(\"ğŸ” Check GPU memory and try reducing batch size\")\n",
        "\n",
        "    # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªØ´Ø®ÙŠØµ\n",
        "    print(f\"\\nğŸ“Š System info:\")\n",
        "    print(f\"   GPU memory used: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
        "    print(f\"   GPU memory cached: {torch.cuda.memory_reserved() / 1e9:.1f} GB\")"
      ],
      "metadata": {
        "id": "NFrc9b9KhxMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "T8QHNFApqsDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final experiment after careful consideration:\n",
        "Due to our chosen settings and following logical reasoning, using more steps and achieving greater stability will result in the model running 700 steps in the final optimization.\n",
        "\n",
        " Ø¨Ø³Ø¨Ø¨ Ø§Ø®ØªÙŠØ§Ø± Ø§Ø¹Ø¯Ø§Ø¯Ø§ØªÙ†Ø§ Ø·Ù„Ø¨Ø¹Ø§ Ø§Ù„Ù…Ù†Ø·Ù‚ Ø¨Ù‚ÙˆÙ„ Ø³ØªÙŠØ¨Ø³ Ø§ÙƒØ«Ø± Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§ÙƒØ«Ø± Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¨Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø®ÙŠØ± Ø±Ø­ ÙŠØ¹Ù…Ù„ Ø¹ 700 Ø³ØªÙŠØ¨"
      ],
      "metadata": {
        "id": "4SQzlmO0b70c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø§Ù„Ù…ÙƒØ§ØªØ¨ ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…ÙˆØµÙˆÙ Ø·Ø¨Ø¹Ø§ ÙˆØ§Ø¬Ù‡Ù†Ø§ Ø´ÙˆÙŠÙ‡ Ù…Ø´Ø§ÙƒÙ„ ÙÙŠ Ø­Ø§Ù„ Ø­Ù…Ù„Ù†Ø§ Ø§Ù†Ø³ÙˆÙ„ÙˆØ« Ø¨Ø¹Ø¯ Ù…ÙƒØ§ØªØ¨ Ù…Ø¹ÙŠÙ†Ù‡ Ù‚Ø®Ù„ÙŠÙ†Ø§Ø®Ø§ Ø§ÙˆÙ„ Ø§Ø´ÙŠ ÙˆØ«Ø§Ù†ÙŠØ§ ÙÙ„Ø§Ø´ Ø§ØªÙ†Ø´ÙŠÙ† Ø¨Ø³Ø¨Ø¨ ØªØ¹Ø§Ø±Ø¶ ÙØ¹Ù…Ù„Ù†Ø§Ù‡ Ø§ÙˆÙ Ø§Ùˆ Ø¹Ø·Ù„Ù†Ø§Ù‡"
      ],
      "metadata": {
        "id": "kTt8wKfoce61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"DISABLE_FLASH_ATTN\"] = \"1\"\n",
        "\n",
        "!pip install --no-deps --upgrade unsloth\n",
        "!pip install --upgrade transformers datasets accelerate peft trl\n",
        "!pip install bitsandbytes\n",
        "!pip install tyro unsloth_zoo xformers\n",
        "!pip install --upgrade huggingface_hub\n",
        "!pip install torch==2.7.0 torchvision==0.18.0 torchaudio==2.7.0 --force-reinstall\n",
        "\n",
        "\n",
        "print(\"âœ… Libraries installed successfully\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "U7pW9lfMrskb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "#Import libraries in the correct order  Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„ØµØ­ÙŠØ­\n",
        "# ===============================\n",
        "\n",
        "#  # Always import Unsloth first to avoid many issues  Ø¯Ø§ÙŠÙ…Ø§ Ø§Ù†Ø³ÙˆÙ„ÙˆØ« Ø§ÙˆÙ„ ÙƒØ«ÙŠØ± Ù…Ø´Ø§ÙƒÙ„\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# The remaining libraries  Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\n",
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "import gc\n",
        "\n",
        "print(\"âœ… All imports successful\")"
      ],
      "metadata": {
        "id": "xs4H-0f-sLLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    max_seq_length=1024,
             # Suitable sequence length for 7B-13B models\n",
        "    #Ù‡Ùˆ Ø·ÙˆÙ„ Ù…Ù†Ø§Ø³Ø¨ Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª 7B-13B\n",
        "    dtype=\"bfloat16\", 
             # Optimal choice for speed and precision on GPU A100\n",
        "    # Ø§ÙƒØ«Ø± Ø´ÙŠØ¡ Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ø³Ø±Ø¹Ù‡ ÙˆØ§Ù„Ø¯Ù‚Ù‡ Ø§Ù„ÙŠ Ø¨Ø¯ÙˆØ± Ø¹Ù„ÙŠÙ‡Ø§ Ù…Ø¹ GPU A100\n",
        "    load_in_4bit=True, 
             # Load in 4-bit to compress weights; improves efficiency without harming accuracy\n",
        "    #Ø¶ØºØ·Ù†Ø§ Ø§Ù„Ø§ÙˆØ²Ø§Ù†  ØªØ¹Ù…Ù„ Ø§Ù‚ÙˆÙ‰  Ø¨Ø³ Ù„Ø§Ù†Ù‡Ø§ Ø§ØµØºØ± Ø¨ØªØ³ØªÙ‡Ù„Ùƒ  Ø§Ù‚Ù„ Ø¨Ù…Ø§ Ø§Ù†Ù‡ Ø§Ø­Ù†Ø§ Ø¹Ù„Ù‰ ÙƒÙˆÙ„Ø§Ø¨ Ù…Ø´ Ø¹Ø¨ÙŠØ¦Ù‡ Ù…Ø®ØµØµÙ‡ Ù…Ø­Ù„ÙŠÙ‡ ÙˆÙ‚ÙˆÙŠÙ‡ ÙƒÙØ§ÙŠÙ‡ Ù‚Ø±Ø±Ù†Ø§ Ø§Ù„Ø§Ø´ÙŠ Ù„Ø§Ù†Ù‡ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù…Ù„Ø§Ø¦Ù… ÙˆÙ…Ø§ Ø¨Ø§Ø«Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ù‚Ù‡\n",
        "    attn_implementation=\"eager\", 
             # Use eager attention for stability; avoids file compatibility issues and keeps training natural\n",
        "    #(Ø§Ø³ØªÙ‚Ø±Ø§Ø±) Ù‡ÙˆÙ† ÙƒÙ†Ø§ Ù…Ù†Ù‚Ø¯Ø± Ù†Ø®ØªØ§Ø± Ø§ÙˆØ¨ØªÙ…Ø§ÙŠØ²Ø± Ø§Ùˆ Ø§ØªÙŠÙ†Ø´ÙŠÙ† Ø³Ø±ÙŠØ¹ Ø¨Ø³ Ø¨Ù…Ø§ Ø§Ù†Ù‡ ÙƒÙ„ Ø§Ø¹Ø¯Ø§Ø¯Ø§ØªÙ†Ø§ Ø¯ÙˆØ±Øª Ø¹Ø§Ù„Ø³Ø±Ø¹Ù‡ ÙˆØ´ÙˆÙŠÙ‡ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ù…Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª ÙˆØ¹Ø¯Ù… Ø§Ù„ØªÙˆØ§ÙÙ‚ Ù‚Ø±Ø±Ù†Ø§ Ù†Ø´ØªØºÙ„ Ø¹Ù„Ù‰ ØªØ¹Ù„ÙŠÙ… Ø·Ø¨ÙŠØ¹ÙŠ\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model( # Apply LoRA (Low Rank Adaptation)\n",
        "    model,\n",
        "    r=8,\n",
        "    # This parameter controls the strength of LoRA adaptation. Higher values increase strength but require more GPU memory; lower values decrease strength. We tried 2, then 4, then 8 and settled.\n",
        "    #Ù‡Ø§Ø¶ Ø¨Ø²ÙŠØ¯ Ø§Ù„Ù‚ÙˆÙ‡ Ø§Ùˆ Ø¨Ù†Ù‚ØµÙ‡Ø§ Ø­Ø³Ø¨ Ø§Ù„Ø¹Ø¯Ø¯ Ù…Ù†Ø­ÙƒÙŠ Ø¹Ù† Ù…ØµØ·Ù„Ø­ Ø§Ø³Ù…Ù‡ ØªÙˆÙ„ÙŠÙŠÙ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠ\n",
        "    #Ø¨Ø³ Ø·Ø¨Ø¹Ø§ ÙƒÙ„ Ù…Ø§ Ø²Ø§Ø¯ ÙƒØ§Ù† Ø¨Ø¯Ù‡ ÙƒØ±Øª Ø¹Ø§Ù„ÙŠ ÙˆÙƒÙ„ Ù…Ø§ Ù‚Ù„ ÙƒØ§Ù† Ø§Ø¶Ø¹Ù Ø¨Ø¯ÙŠÙ†Ø§ Ø¨ 2 Ø«Ù… 4 Ø«Ù… 8 ÙˆÙˆÙ‚ÙÙ†Ø§\n",
        "\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    # Target the main transformer projection layers for LoRA: Query, Key, Value, Output. These are where the model focuses on understanding and processing data.\n",
        "    # ÙÙŠ Ø¨ÙƒÙ„ Ù…ÙˆØ¯ÙŠÙ„ Ø·Ø¨Ù‚Ø§Øª ØªØ¹Ù„Ù…Ù†Ø§ Ø¹Ù†Ù‡Ø§ Ø§Ø³Ù…Ù‡Ø§ Ù„Ø§ÙŠØ±Ø² Ø¨ÙƒÙ„ Ù„Ø§ÙŠØ± ÙÙŠÙ‡Ø§ Ù…Ø¹Ø§Ø¯Ù„Ø§ ÙˆØ­Ø³Ø§Ø¨Ø§Øª Ø§Ù†ØªÙŠ Ø¨Ø¯Ùƒ ØªØ®ØªØ§Ø± Ø§Ù„Ø·Ø¨Ù‚Ù‡ Ø§Ù„Ù…Ù„Ø§Ø¦Ù…Ù‡ Ø§Ù„ÙŠ Ø¨Ø¯Ùƒ ØªØ´ØªØºÙ„ Ø¹Ù„ÙŠÙ‡Ø§ Ø¨Ø§Ù„Ù„ÙˆØ±Ù„Ø§\n",
        "    # Ø·Ø¨Ø¹Ø§ Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø·Ø¨ÙŠÙŠØ¹ÙŠ Ø¨ÙƒÙˆÙ† Ù„Ù‡Ø°ÙˆÙ„ Ø§Ù„ 4    ÙƒÙŠÙˆ Ù…Ù† ÙƒÙˆÙŠØ±ÙŠ Ø§Ø³ØªØ¹Ù„Ø§Ù…  Ø§Ù„ÙƒÙŠ Ù…Ù† ÙƒÙŠÙŠ Ù…ÙØªØ§Ø­  Ø§Ù„ÙÙŠ Ù…Ù† ÙØ§Ù„ÙŠÙˆ ÙˆØ§Ù„Ø§ÙˆÙˆÙˆ Ù…Ù† Ø§ÙˆØªØ¨ÙˆØª Ù…Ø®Ø§Ø±Ø¬\n",
        "    #ÙØ·Ø¨ÙŠØ¹ÙŠ Ù†Ø³ØªÙ‡Ø¯Ù Ù‡Ø§ÙŠ Ø§Ù„Ø·Ø¨ÙØ§Øª Ù„Ø§Ù†Ù‡  Ù‡Ø§ÙŠ Ù‡ÙŠ Ø¹Ù‚Ù„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙˆÙƒÙŠÙ Ø¨ÙÙ‡Ù… ÙˆØ¨Ø±ÙƒØ² Ø¨Ø§Ù„Ø¯Ø§ØªØ§\n",
        "\n",
        "    lora_alpha=16,\n",
        "    # Alpha scales the LoRA update; 16 is a balanced value suitable for this model and dataset.\n",
        "    # Ù…Ø«Ù„ Ù…Ø§ Ù…Ù†Ù‚Ø¯Ø± Ù†Ø´Ø±Ø­ Ø§Ù†Ù‡ Ø§Ù„Ø§Ø± 8 Ù‡ÙŠ Ø­Ø±ÙŠÙ‡ Ø§Ù„ØªØ¹Ø¯Ø¨Ù„ Ø¹Ù„Ù‰ Ø§ÙˆØ²Ø§Ù† Ø§Ù„Ù„ÙˆØ±Ø§ Ù…Ù†Ù‚Ø¯Ø± Ù†Ù‚ÙˆÙ„ Ø¹Ù† Ø§Ù„Ø§Ù„ÙØ§\n",
        "    #Ø§Ù†Ù‡ Ù‚ÙˆÙ‡ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ ÙŠØ¹Ù†ÙŠ Ù…Ø¹Ùƒ Ø­Ø±ÙŠÙ‡ ØªØ¹Ø¯Ù„ Ù‡Ø§Ø¶ Ø§Ù„ÙˆØ²Ù† Ø¨Ø³ Ù‚Ø¯ÙŠØ´ ØªØ¹Ø¯Ù„Ù‡ Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù… 16 Ø²ÙŠ Ø¯ÙŠÙÙˆÙ„Øª Ù…ØªÙÙ‚ Ø¹Ù„ÙŠÙ‡ Ø§Ù†Ù‡ Ù…Ø¹Ø¯Ù„ Ù…ØªÙˆØ§Ø²Ù† Ø·Ø¨Ø¹Ø§\n",
        "    #  Ø§Ù„Ø§Ø´ÙŠ Ø¨ØªØºÙŠØ± Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙˆØ­Ø³Ø§Ø³ÙŠØªÙ‡ Ù…Ù† Ø§Ù„Ù…Ù‡Ù…Ù‡ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø³ 16 Ø¨Ø­Ø§Ù„ØªÙ†Ø§ Ø¬Ø¯Ø§ Ù…Ù†Ø§Ø³Ø¨\n",
        "\n",
        "    lora_dropout=0.0,\n",
        "    # Dropout for LoRA; here set to 0 because our model does not overfit in current experiments.\n",
        "    # ØªØ³ØªØ¹Ù…Ù„ Ø§ÙƒØ«Ø± ÙÙŠ Ø­Ø§Ù„Ø§Øª Ù…Ù†Ø¹ ÙˆÙ…Ø³Ø§Ø¹Ø¯Ù‡ Ø¶Ø¯ Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø§ÙˆÙÙŠØ±ÙÙŠØªÙŠÙ†Ø¬ ÙŠØ¹Ù†ÙŠ Ø¨ØªØ¹Ø·ÙŠ Ù†Ø³Ø¨Ù‡ Ù…Ø¹ÙŠÙ†Ù‡ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚ÙŠÙ…Ù‡ ØµÙØ±  Ø¨Ø´ÙƒÙ„ Ø¹Ø´ÙˆØ§Ø¦ÙŠ ÙƒÙ„ Ù…Ø±Ù‡ Ù…Ø´Ø§Ù† ØªÙ‚Ù„Ù„ Ù†Ø³Ø¨Ù‡ Ø§Ù„ØªØ¹Ù…ÙŠÙ…\n",
        "    #Ø¨Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¨Ø³ Ø·Ø¨Ø¹Ø§ ÙÙŠ Ø­Ø§Ù„Ù‡ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ØªØ§Ø¹Ù†Ø§ Ø¨Ø§Ù„ØªØ¬Ø§Ø±Ø¨ Ø§Ù„Ø§Ø®ÙŠØ±Ù‡ ÙƒØ§Ù† Ø§Ù„ÙˆØ¶Ø¹ ØªÙ…Ø§Ù… ÙÙØ´ Ø­Ø§Ø¬Ù‡ ÙˆÙ‡Ùˆ Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù… Ù…Ø§ Ø¨ÙŠØ³ØªØ¹Ù…Ù„ Ø¨Ø§Ù„Ù„ÙˆØ±Ø§ Ø¨ÙŠØ³ØªØ¹Ù…Ù„ Ø§ÙƒØ«Ø± Ø¨Ù…ÙˆØ¯ÙŠÙ„Ø§Øª ÙƒØ§Ù…Ù„Ù‡\n",
        "\n",
        "    bias=\"none\",\n",
        "    # Keep bias unchanged; we donâ€™t want to shift outputs globally.\n",
        "    #  Ø§Ø®ØªØ±Ù†Ø§  Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¯ÙŠÙÙˆÙ„Øª Ù„Ø§Ù†Ù‡ Ù‡ÙˆÙ† Ø§Ù†ØªÙŠ Ø¨ØªØ³Ø§Ù„  Ø§Ø°Ø§ Ø¨Ø¯Ùƒ Ø§Ù†Ù‡ Ø¨Ø¹Ø¯ ÙƒÙ„ ØªØºÙŠÙŠØ± Ù†Ø­Ø±Ùƒ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙƒÙ„Ù‡Ø§  Ø¨Ù‚ÙŠÙ…Ù‡ Ù…Ø¹ÙŠÙ†Ù‡ ÙˆÙ‡Ø§Ø¶ Ù…Ø´ Ù‡Ø¯ÙÙ†Ø§\n",
        "\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    # Enable gradient checkpointing to save memory; unsloth implementation is faster and compatible with this model.\n",
        "    #Ø§Ù„Ù…ÙˆØ¯Ù„ ØªØ¹Ø§Ù†Ø§ ÙƒØ¨ÙŠØ± ÙˆÙ…Ø´ Ù…Ù†Ø·Ù‚ÙŠ  Ù†Ø­ÙØ¸ ÙƒÙ„ Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª ÙˆØ§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª\n",
        "    #Ø§Ø­Ù†Ø§ Ù…Ù†Ø®Ø²Ù† Ù‚Ø³Ù…  ÙˆØ§Ø°Ø§ Ø§Ø­ØªØ¬Ù†Ø§ Ø§Ù„Ø¨Ø§Ù‚ÙŠ Ù…Ù†Ø­Ø³Ø¨Ù‡  ÙˆØ·Ø¨Ø¹Ø§ Ø§Ø®ØªØ±Ù†Ø§ ØªØ¨Ø¹ Ø§Ù†Ø³ÙˆÙ„ÙˆØ« Ø§ÙƒÙŠØ¯ Ù„Ø§Ù†Ù‡ Ø§Ø³Ø±Ø¹ ÙˆÙ„Ø§Ù†Ù‡ Ù…ÙˆØ¯ÙŠÙ„Ù†Ø§ ÙƒØ§Ù…Ù„ Ù…Ù† Ø§Ù†Ø³ÙˆÙ„ÙˆØ«\n",
        ")\n",
        "\n",
        "# 3ï¸âƒ£ Preparing the dataset\n",
        "print(\"ğŸ“Š Preparing data...\")\n",
        "#ğŸ“Š ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...\n",
        "\n",
        "dataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\").select(range(50000))\n",
        "print(f\"ğŸ“š Loaded {len(dataset):,} examples\")\n",
        "#ğŸ“š ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(dataset):,} Ù…Ø«Ø§Ù„\n",
        "\n",
        "def format_conversations(example):\n",
        "    conv = example[\"conversations\"]\n",
        "    text = \"<|begin_of_text|>\"\n",
        "\n",
        "    for turn in conv:\n",
        "        role = turn.get(\"from\", turn.get(\"role\", \"\"))\n",
        "        content = turn.get(\"value\", turn.get(\"content\", \"\"))\n",
        "\n",
        "        if role in [\"human\", \"user\"]:\n",
        "            text += f\"<|start_header_id|>user<|end_header_id|>\\n\\n{content}<|eot_id|>\"\n",
        "        elif role in [\"gpt\", \"assistant\"]:\n",
        "            text += f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{content}<|eot_id|>\"\n",
        "\n",
        "    return {\"text\": text}\n",
        "\n",
        "dataset = dataset.map(format_conversations)\n",
        "dataset = dataset.filter(lambda x: len(x[\"text\"]) > 100)\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "train_dataset = dataset.select(range(train_size))\n",
        "eval_dataset = dataset.select(range(train_size, len(dataset)))\n",
        "\n",
        "print(f\"âœ… Training: {len(train_dataset)}, Eval: {len(eval_dataset)}\")\n",
        "#âœ… ØªØ¯Ø±ÙŠØ¨: {len(train_dataset)}, ØªÙ‚ÙŠÙŠÙ…: {len(eval_dataset)}\n",
        "\n",
        "# Setting up training\n",
        "print(\"ğŸ‹ï¸ Setting up training...\")\n",
        "#ğŸ‹ï¸ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨...\n",
        "\n",
        "from transformers import TrainingArguments, TrainerCallback\n",
        "\n",
        "class MovingAverageEvalLossCallback(TrainerCallback):\n",
        "    def __init__(self, window_size=10):\n",
        "        self.window_size = window_size\n",
        "        self.eval_losses = []\n",
        "\n",
        "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "        if metrics and \"eval_loss\" in metrics:\n",
        "            self.eval_losses.append(metrics[\"eval_loss\"])\n",
        "            self.eval_losses = self.eval_losses[-self.window_size:]\n",
        "            avg = sum(self.eval_losses) / len(self.eval_losses)\n",
        "            print(f\"\\nğŸ”¹ Average of last {len(self.eval_losses)} eval_loss: {avg:.4f}\\n\")\n",
        "            #ğŸ”¹ Ù…ØªÙˆØ³Ø· Ø¢Ø®Ø± {len(self.eval_losses)} eval_loss: {avg:.4f}\\n\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./llama3_intensive\",\n",
        "    per_device_train_batch_size=2,\n",
        "    # Number of training samples per device. Higher values are better for speed but limited by GPU memory and cost.\n",
        "    #Ø®Ù„Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§ÙƒÙ… Ø¹ÙŠÙ†Ù‡ Ø¨ØªÙ†Ø¨Ø¹Ø«  Ø¨ÙˆÙ‚Øª ÙˆØ§Ø­Ø¯ ÙƒÙ„ Ù…Ø§ Ø²Ø§Ø¯ Ø§Ø­Ø³Ù† Ø¨Ø³ ÙƒØ«ÙŠØ± ÙƒØ±Øª Ø´Ø§Ø´Ù‡ ÙˆØ§Ø­Ù†Ø§ Ù…Ù†Ø¯ÙØ¹ Ù…ØµØ§Ø±ÙŠ Ù‚Ù„ÙŠÙ„ Ù„Ù„ØªÙˆÙÙŠØ± ...\n",
        "\n",
        "    gradient_accumulation_steps=8,\n",
        "    # Accumulate gradients over 8 steps before updating weights. Saves memory and reduces compute cost.\n",
        "    # ØªØ­Ø¯ÙŠØ«  Ø§Ù„ÙˆØ§ÙŠØªØ² Ø§Ù„Ø§ÙˆØ²Ø§Ù† Ø§Ù„ÙŠ Ù…Ù†Ù„Ù‚Ø§Ù‡Ø§ Ø¨Ø¯Ù„ Ù…ÙŠÙƒÙˆÙ† Ø¨Ø¹Ø¯ ÙƒÙ„ Ø¨Ø§ØªØ´ Ù„Ø§ Ø¬Ù…Ø¹ ÙƒÙ„ 8 Ø¹Ø¯Ù„  Ø¨Ø¹Ø¯Ù‡Ù†  Ø¨ÙˆÙØ± Ù…ØµØ§Ø±ÙŠ ÙˆØ§Ø³ØªÙ‡Ù„Ø§Ùƒ\n",
        "\n",
        "    per_device_eval_batch_size=2,\n",
        "    # Number of evaluation samples per device; kept same as training batch size for consistency.\n",
        "    # Ù†ÙØ³ Ø§Ù„ØªØ±Ø§ÙŠÙ† Ø¨Ø³ Ø§Ù„Ø§ÙƒÙŠØ¯ Ø§Ù†Ù‡ Ù‡ÙˆÙ† Ø­Ø³Ø¨ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù…  Ø±Ù‚Ù… ØµØºÙŠØ±  Ø·Ø¨Ø¹Ø§ Ø§Ø®ØªØ±Ù†Ø§ ÙŠÙƒÙˆÙ† Ø²ÙŠ Ø§Ù„ØªØ±Ø§ÙŠÙ† 2\n",
        "\n",
        "    learning_rate=1e-4,\n",
        "    # Learning rate chosen for LoRA on 4-bit LLaMA models; balances training speed, stability, and convergence.\n",
        "    # Ø¯ÙˆØ±Ù†Ø§ Ø¹Ø³Ø±Ø¹Ù‡ ØªØ¯Ø±ÙŠØ¨ Ù…Ù„Ø§Ø¦Ù…Ù‡ Ù„Ù„ÙˆØ±Ø§ Ù„Ù…ÙˆØ¯ÙŠÙ„Ø² Ù„Ø§Ù…Ø§  ÙˆØ®ØµÙˆØµØ§ Ù…Ø¹ 4Ø¨ÙŠØª ÙƒØ§Ù† Ø¹Ù†Ø§ Ø¹Ø¯Ù‡ Ø§Ø®ØªÙŠØ§Ø±Ø§Øª ÙˆÙ‡Ø§Ø¶ Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø§ÙˆØ³Ø· Ù„Ø§ Ø³Ø±ÙŠØ¹ ÙƒØ«ÙŠØ± ÙˆÙ„Ø§ Ø¨Ø·Ø¦ÙŠ Ù…Ø³ØªÙ‚Ø± ÙˆÙ…Ù†Ø§Ø³Ø¨\n",
        "\n",
        "    warmup_steps=100,\n",
        "    # Gradually increase learning rate over 100 steps to avoid large weight jumps at the start.\n",
        "    # Ù‡Ø§ÙŠ Ù…Ø¹Ù†Ø§Ù‡Ø§ Ø§ÙƒÙ… Ø³ØªÙŠØ¨ ÙŠÙˆØ®Ø° Ù…Ø¹Ù‡ ØªØªÙˆØµÙ„ Ø³Ø±Ø¹Ù‡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨  Ù„Ù„Ø±Ù‚Ù… Ø§Ù„ÙŠ Ø­Ø·ÙŠÙ†Ø§Ù‡ Ù…Ø´ Ù…Ø±Ù‡ ÙˆØ­Ø¯Ù‡ ÙˆÙŠØµÙŠØ± Ø¹Ù†Ø§ Ø§ÙˆØ²Ø§Ù†   Ø¨ÙØ¬ÙˆØ§Øª ÙƒØ¨ÙŠØ±Ù‡ Ù…Ù† Ø§ÙˆÙ„Ù‡Ø§  100 Ø§Ùˆ Ø­ØªÙ‰ 50 Ù…Ù„Ø§Ø¦Ù…\n",
        "\n",
        "    max_steps=700,\n",
        "    # Total number of training steps. 700 chosen based on stability and efficiency; beyond this point gains are negligible.\n",
        "    # Ø¹Ø¯Ø¯ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø­ØªÙ‰ Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø±Ù‚Ù… 700 Ø§Ø¬Ø§ Ø¨Ø¹Ø¯ ØªÙÙƒÙŠØ± ÙˆØªØ¬Ø±Ø¨Ù‡  Ø®ØµÙˆØµØ§ Ø¹Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø¹Ø§Ø¯Ø§Ø¯Ø§Øª Ù…Ø³ØªÙ‚Ø±Ù‡ ÙˆØ¨Ø±Ø¶Ùˆ Ø¨Ù†Ø¸Ø§Ù… Ø§Ù„ÙƒÙˆØ³ÙŠÙ†  Ø§Ù„ÙŠ\n",
        "    # Ø¨Ø§Ù„Ø§ÙˆÙ„ Ø¨ÙƒÙˆÙ† Ù‚ÙˆÙŠ Ø¨Ø§Ù„Ø§Ø®Ø± Ø¨ØµÙŠØ± Ø¶Ø¹ÙŠÙ ÙˆØ¨Ù…Ø§ Ø§Ù†Ù‡ Ù†Ù…ÙˆØ°Ø¬Ù†Ø§  Ø¨Ø§Ù„Ø§Ø®Ø± Ù…Ø³ØªÙ‚Ø± Ø§Ø°Ø§ ÙƒÙ…Ù„Ù†Ø§ Ù„Ù„ 1000 Ø¨ÙƒÙˆÙ† Ø¨Ù„Ø§ ÙØ§ÙŠØ¯Ø® ÙˆÙƒÙ…Ø§Ù† Ù…ÙˆØ§Ø±Ø¯ Ø¶Ø§ÙŠØ¹Ù‡ ÙÙ‚Ø±Ù†Ù†Ø§ Ù†ÙˆÙ‚Ù Ø¹Ù†Ø¯ 7--\n",
        "\n",
        "    eval_steps=100,\n",
        "    # Evaluate model every 100 steps.\n",
        "    # ÙƒÙ„ 100 Ø®Ø·ÙˆÙ‡ Ø¨Ø¹Ù…Ù„ ØªÙ‚ÙŠÙŠÙ…  Ø§Ø®Ø±Øª Ù…ÙŠÙ‡ Ù„Ø§Ù†Ù‡ Ø¨ØªÙˆÙÙŠØ´ ÙƒÙ„ 50 Ø¨Ù‚Ø¹Ø¯ 10 Ø³Ø§Ø¹Ø§Øª Ø¹ 100 ÙˆØ·Ù„Ø¹Øª Ø±ÙˆØ­Ù†Ø§ Ù‡Ù‡Ù‡\n",
        "\n",
        "    save_steps=200,\n",
        "    # Save checkpoint every 200 steps to prevent loss of progress due to Colab interruptions.\n",
        "    # Ø§Ù„ÙƒÙˆÙ„Ø§Ø¨ Ù…Ø´ÙƒÙ„Ø¬Ù‡ Ø§Ù† Ù‚Ø·Ø¹ Ø§Ù„Ø§Ù†ØªØ±Ù†Øª Ø®Ù„ØµØª Ø§Ù„Ø¨Ø·Ø§Ø±ÙŠÙ‡ Ø§Ùˆ ØµØ§Ø± Ø±ÙŠØ³ØªØ§Ø±Øª Ø§Ùˆ Ø§ÙŠ Ø§Ø´ÙŠ ÙƒÙ„Ù‡ Ø¨Ø±ÙˆØ­ ÙÙ‚Ø±Ø±Ù†Ø§ Ù†Ø­ÙØ¸ Ø§Ù„Ø³ØªÙŠØ¨Ø³ ÙƒÙ„ 200 Ø®Ø·ÙˆÙ‡ Ù…Ø´Ø§Ù†  Ù…ÙŠØ¶ÙŠØ¹Ø´ Ø§Ù„ØªØ¹Ø¨\n",
        "\n",
        "    logging_steps=25,\n",
        "    # Log metrics every 25 steps.\n",
        "    # ÙƒÙ„ 25 Ø®Ø·ÙˆØ© Ù†Ø³Ø¬Ù„ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª\n",
        "\n",
        "    optim=\"adamw_8bit\",\n",
        "    # Use 8-bit AdamW optimizer for speed and stability. Efficient for large models and reduces memory footprint.\n",
        "    # Ø§Ø¯Ø§Ù… Ø¯Ø¨Ù„ÙŠÙˆ Ù†Ø³Ø®Ù‡ Ù…Ø¹Ø¯Ù„Ù‡ Ù…Ù† Ø§Ø¯Ù… Ø·Ø¨Ø¹Ø§ Ù‡Ø§Ø¶ Ø§Ù„Ø¬ÙˆØ±ÙŠØ«Ù… ØªØºÙŠÙŠØ± Ø§ÙˆØ²Ø§Ù† Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø­Ù„Ùˆ ÙÙŠÙ‡ Ø§Ù†Ù‡ Ø§ÙˆØ¨ØªÙ…Ø§ÙŠØ±Ø² Ø³Ø±ÙŠØ¹\n",
        "    # ÙˆØ¨Ø§Ø®ØªÙŠØ§Ø±Ù†Ø§ Ù„Ù„ Ø¯Ø§Ø¨Ù„ÙŠÙˆ Ù…Ø¹ 8 Ø¨ÙŠØª Ø®Ù„ÙŠÙ†Ø§Ù‡ Ø§Ø³Ø±Ø¹ Ø¨ÙƒØ«ÙŠØ± ÙˆÙ…Ø³ØªÙ‚Ø± Ø·Ø¨Ø¹Ø§ Ø§Ù„Ø§Ø´ÙŠ Ø§Ø¬Ø§ Ø¹Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ù‡ Ø¨Ø³ Ù…Ø§ ÙÙƒØ±Ù†Ø§ ÙƒØ«ÙŠØ±  Ù„Ø§Ù†Ù‡ Ø§Ù„Ø®Ø³Ø§Ø±Ù‡ Ø¨Ø§Ù„Ø¯Ù‚Ù‡ Ø¬Ø¯Ø§ Ù…Ù‡Ù…Ø´Ù‡\n",
        "\n",
        "    weight_decay=0.01,\n",
        "    # Regularization to prevent extreme weights and overfitting.\n",
        "    # Ø§Ù„Ø§Ø® Ù‡ÙˆÙ† Ø¨ØªØ­ÙƒÙ… Ø¨ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø§ÙˆØ²Ø§Ù† Ø§Ø°Ø§ Ù‡Ù„ÙŠÙ†Ø§Ù‡ Ø¹Ø±Ø§Ø­ØªÙ‡ Ù…Ù…ÙƒÙ† ÙŠØµÙŠØ± Ø¹Ù†Ø§ Ø§ÙˆØ²Ø§Ù† ÙƒØ«ÙŠØ± ÙƒØ¨ÙŠØ±Ù‡ Ø§Ùˆ ÙƒØ«ÙŠØ± ØµØºÙŠØ±Ù‡ ÙÙ‡ÙˆÙ† Ø¹Ù…Ù„Ù†Ø§ Ø±ÙŠØ¬ÙŠÙˆÙ„Ø§Ø±ÙŠØ²ÙŠØ´ÙŠÙ†\n",
        "\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    # Cosine learning rate schedule: starts high, ends low; improves stability over training.\n",
        "    # Ø´Ø±Ø­Ù†Ø§ Ø¹Ù†Ù‡ Ø·Ø±ÙŠÙ‚Ù‡ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙŠ Ø¨ØªÙƒÙˆÙ† Ù‚ÙˆÙŠÙ‡ Ø¨Ø§Ù„Ø§Ø¨Ø¯Ø§ÙŠÙ‡ ÙˆØ¨Ø§Ù„Ù†Ù‡Ø§ÙŠÙ‡ ØµØºÙŠØ±Ù‡ ÙˆØ¨ØªØ¯ÙˆØ± Ø¹Ø§Ø³ØªÙ‚Ø±Ø§Ø±\n",
        "\n",
        "    eval_strategy=\"steps\",\n",
        "    # Evaluate model at regular step intervals.\n",
        "    # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙƒÙ„ Ø¹Ø¯Ø¯ Ù…Ø­Ø¯Ø¯ Ù…Ù† Ø§Ù„Ø®Ø·ÙˆØ§Øª\n",
        "\n",
        "    save_strategy=\"steps\",\n",
        "    # Save model at regular step intervals.\n",
        "    # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙƒÙ„ Ø¹Ø¯Ø¯ Ù…Ø­Ø¯Ø¯ Ù…Ù† Ø§Ù„Ø®Ø·ÙˆØ§Øª\n",
        "\n",
        "    load_best_model_at_end=True,\n",
        "    # Load the best checkpoint at the end based on evaluation metric.\n",
        "    # Ù…Ø´ Ù…Ù†Ø­ÙØ¸ Ø§Ø®Ø± Ù†Ø³Ø®Ù‡ Ø¨Ø³ Ù„Ø§ Ù…Ù†Ø´ÙˆÙ Ø§Ø­Ø³Ù† Ù†Ø³Ø®Ù‡ Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ ØªÙƒÙˆÙ† Ø§Ù„Ø§Ø®ÙŠØ±Ù‡ Ø§Ùˆ Ù‚Ø¨Ù„ Ø§Ù„Ø§Ø®ÙŠØ±Ù‡ Ø§Ø°Ø§ Ù…ØµØ§Ø±Ø´ Ù…Ø´Ø§ÙƒÙ„\n",
        "\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    # Use evaluation loss as the metric to select the best model.\n",
        "    # Ø·Ø¨Ø¹Ø§ Ø¨Ù…Ø§ Ø§Ù†Ù‡ Ø§Ù„Ù†Ù…ÙˆØ¶Ø¬ Ø¨Ù†Ù‚Ø§Ø³ Ø¹Ù„ Ø§Ù„Ø§ÙŠÙØ§Ù„ Ù„ÙˆØ³  ÙˆÙ‚ÙˆØªÙ‡ Ø¨ØªØªØ¹Ø¨Ø± Ø¹Ù† Ø·Ø±ÙŠÙ‚Ù‡ Ù…Ø´ Ø¹Ù† Ø§Ù„ØªØ±Ø§ÙŠÙ†ÙŠÙ†Ø¬ Ù„ÙˆØ³ ÙÙ‡Ùˆ Ø§Ù„ÙŠ Ø¨Ù‡Ù…Ù†Ø§\n",
        "\n",
        "    greater_is_better=False,\n",
        "    # Lower eval_loss is better for our model.\n",
        "    #Ù„Ø§  Ø¨Ù†Ù…ÙˆØ°Ø¬Ù†Ø§ Ø§Ù„Ù…ØªÙˆØ§Ø¶Ø¹ Ø§Ù„Ø§Ù‡Ù… Ø¹Ù†Ø§ Ù‡Ùˆ Ø§Ù„Ù„ÙˆØ³ Ø­Ø§Ù„ÙŠØ§ Ø¨Ø³ Ø·Ø¨Ø¹Ø§ Ù…Ø¹ Ø§Ù„ÙˆÙ‚Øª Ù…Ù†ØµÙŠØ± Ù†Ø¯ÙˆØ± Ø¹Ø§Ù„Ø¯Ù‚Ù‡ Ø§ÙƒØ«Ø±\n",
        "\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    # Use bf16 precision for faster training with A100 GPU while maintaining stability.\n",
        "    #Ø´Ø±Ø­Øª Ø§Ù†Ù‡ Ø§Ø®ØªØ±Ù†Ø§Ù‡ Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„ØªØ´ÙŠØ¨ Ø§Ù„ÙŠ Ø¹Ù†Ø§ ÙˆØªÙˆØµÙŠØ§Øª Ø´Ø±ÙƒÙ‡ Ù†ÙŠÙÙŠØ¯ÙŠØ§ Ø¹Ù…ÙˆÙ‚Ø¹Ù‡Ù… Ø¨Ø­ÙŠØ« Ø¨Ø¬Ù…Ø¹ Ø³Ø±Ø¹Ù‡ Ø§Ù„ Ø§Ù Ø¨ÙŠ 16 ÙˆØ§Ù…Ø§Ù† Ø§Ù„ 32\n",
        "\n",
        "    dataloader_num_workers=2,\n",
        "    # Number of workers for data loading; 2 is sufficient for our dataset size.\n",
        "    # Ù…Ø´ Ø¨Ø­Ø§Ø¬Ù‡ Ù„Ø§ÙƒØ«Ø± Ù…Ù† Ø«Ù†ÙŠÙ† Ù„ØªØ­Ø¶Ø± Ø§Ù„Ø¯Ø§ØªØ§ ÙƒÙ„Ù‡Ù† Ø¹Ø¨Ø¹Ø¶Ù‡Ù† 50 Ø§Ù„Ù Ù…Ù† 100 Ø§Ù„Ù\n",
        "\n",
        "    remove_unused_columns=True,\n",
        "    report_to=\"none\",\n",
        "    seed=42,\n",
        "    gradient_checkpointing=True,\n",
        "    save_total_limit=3,\n",
        "    # Keep only the last 3 saved checkpoints to save storage.\n",
        "    # Ø¨Ù…Ø§ Ù†Ø§Ùƒ Ø¨ØªØ­ÙØ¸  Ù†Ø³Ø® Ø¯Ø§ÙŠÙ…Ø§ Ø§Ø­ÙØ¸ Ø§Ø®Ø± 3\n",
        ")\n",
        "\n",
        "trainer_intensive = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=1024,\n",
        "    args=training_args,\n",
        "    packing=False,\n",
        "    # Treat each text individually; do not merge multiple short texts into one.\n",
        "    # Ø¹Ø§Ù…Ù„ ÙƒÙ„ Ù†Øµ Ù„Ø­Ø§Ù„Ù‡ Ø§Ø°Ø§ ÙƒØ§Ù† Ù‚ØµÙŠØ± Ù…ØªØ¯Ù…Ø¬Ø´ ÙƒØ°Ø§ ÙˆØ§Ø­Ø¯ Ù…Ø¹ Ø¨Ø¹Ø¶\n",
        "\n",
        "    callbacks=[MovingAverageEvalLossCallback(window_size=10)],\n",
        "    # Callback to compute moving average of eval loss for easier monitoring.\n",
        "    #  Ø§Ø´ÙŠ ØªØ§ÙÙ‡ ÙÙƒØ±ØªÙ‡ ÙŠØ³Ø§Ø¹Ø¯Ù†ÙŠ Ø·Ù„Ø¹ Ù‡Ø¨Ù„ Ù‡Ù‡Ù‡\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "AOdjpuQ4sMrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø¨Ø¯Ø§ ØªØ±Ø¯ÙŠØ¨\n"
      ],
      "metadata": {
        "id": "hMyh7_UQdNNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Starting INTENSIVE training...\")\n",
        "\n",
        "\n",
        "try:\n",
        "    # Start training\n",
        "    result_intensive = trainer_intensive.train()\n",
        "\n",
        "    print(\"\\nğŸ‰ INTENSIVE TRAINING COMPLETED!\")\n",
        "    print(f\"ğŸ“Š Final training loss: {result_intensive.training_loss:.4f}\")\n",
        "    print(f\"â±ï¸  Training time: {result_intensive.metrics['train_runtime']:.1f} seconds\")\n",
        "\n",
        "    # Save the fine-tuned model\n",
        "    print(\"ğŸ’¾ Saving intensively trained model...\")\n",
        "    model.save_pretrained(\"./llama3_intensive_final\")\n",
        "    tokenizer.save_pretrained(\"./llama3_intensive_final\")\n",
        "\n",
        "    print(\"âœ… Intensive model saved!\")\n",
        "\n",
        "    # Comprehensive testing\n",
        "    print(\"\\nğŸ§ª Testing intensively trained model...\")\n",
        "\n",
        "    FastLanguageModel.for_inference(model)\n",
        "\n",
        "    test_questions = [\n",
        "        \"What do u think about humans \",\n",
        "        \"Explain machine learning in simple terms.\",\n",
        "        \"How do neural networks work?\",\n",
        "        \"What are the benefits of AI?\",\n",
        "        \"Write a short story about a robot.\"\n",
        "    ]\n",
        "\n",
        "    for i, question in enumerate(test_questions, 1):\n",
        "        print(f\"\\nğŸ¤– Test {i}: {question}\")\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        # Extract only the answer\n",
        "        answer = response.split(\"assistant<|end_header_id|>\\n\\n\")[-1]\n",
        "        print(f\"ğŸ’¬ Answer: {answer[:200]}...\")\n",
        "\n",
        "        if i >= 2:  # Test only the first two questions to save time\n",
        "            print(\"   ... (Tests have been reduced to save time\")\n",
        "            break\n",
        "\n",
        "    print(\"\\nğŸŠ INTENSIVE TRAINING COMPLETE!\")\n",
        "    print(\"ğŸ“ Model saved in: ./llama3_intensive_final/\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Training error: {e}\")\n",
        "    print(\"ğŸ” Check GPU memory and try reducing batch size\")\n",
        "\n",
        "    # Diagnostic information\n",
        "    print(f\"\\nğŸ“Š System info:\")\n",
        "    print(f\"   GPU memory used: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
        "    print(f\"   GPU memory cached: {torch.cuda.memory_reserved() / 1e9:.1f} GB\")"
      ],
      "metadata": {
        "id": "YTXDq5n1tMjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run it locally and save the model before Colab says goodbye"
      ],
      "metadata": {
        "id": "4ZhwJ0IedP33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TXVnHhmoUYmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "src = '/content/llama3_intensive_final'\n",
        "dst = '/content/drive/MyDrive/MyLastUpdateForMyLLMProject'\n",
        "\n",
        "if os.path.exists(dst):\n",
        "    shutil.rmtree(dst)\n",
        "\n",
        "shutil.copytree(src, dst)\n",
        "print(f'âœ… Model copied to Google Drive: {dst}')\n"
      ],
      "metadata": {
        "id": "S7AWk5FJuZTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.make_archive('/content/llama3_intensive_final', 'zip', '/content/llama3_intensive_final')\n"
      ],
      "metadata": {
        "id": "O5Mzs6INU_-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload it to GitHub with a push"
      ],
      "metadata": {
        "id": "9FbS40QkdrP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "\n",
        "\n",
        "github_user = \"AliBesher\"\n",
        "github_repo = \"MyLLMProject\"\n",
        "branch = \"main\"\n",
        "token = \"ghpØ¡Ø¡Ø¡Ø¡Ø¡Ø¡Ø¡Ø¡Ø¡Ø¡Ø¡Ø¡Ø¡Ø¡\"\n",
        "\n",
        "\n",
        "import os\n",
        "os.chdir('/content')\n",
        "!rm -rf MyLLMProject\n",
        "\n",
        "clone_url = f\"https://{github_user}:{token}@github.com/{github_user}/{github_repo}.git\"\n",
        "!git clone {clone_url}\n",
        "os.chdir('MyLLMProject')\n",
        "\n",
        "!cp -r /content/llama3_intensive_final/* .\n",
        "\n",
        "\n",
        "!git config --global user.email \"ali.besher12@gmail.com\"\n",
        "!git config --global user.name \"AliBesher\"\n",
        "\n",
        "!git add .\n",
        "!git commit -m \"Add fine-tuned LLaMA3 (zip)\"\n",
        "!git push {clone_url} {branch} --force\n"
      ],
      "metadata": {
        "id": "qAN0QWZbW1gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Push it to Hugging Face"
      ],
      "metadata": {
        "id": "dniC7Veudvp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U huggingface_hub\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(\"hf_zagaBHu\")\n",
        "\n",
        "from huggingface_hub import create_repo\n",
        "create_repo(\"llama3-intensively-finetuned\", private=False)\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "\n",
        "api.upload_folder(\n",
        "    folder_path=\"/content/llama3_intensive_final\",\n",
        "    repo_id=\"AliB17/llama3-intensively-finetuned\",\n",
        "    repo_type=\"model\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "oI5OwByHbNsQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
